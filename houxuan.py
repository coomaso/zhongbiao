import json
import requests
import datetime
import os
import re
import time
from bs4 import BeautifulSoup
from typing import List, Dict, Any

class BidMonitor:
    def __init__(self):
        # åˆå§‹åŒ–æ–‡ä»¶è·¯å¾„
        self.original_file = "hx.json"
        self.parsed_file = "hx_parsed.json"
        
        # ä¼ä¸šå¾®ä¿¡é…ç½®
        self.webhook_url = os.getenv("QYWX_URL")
        self.webhook_zb_url = os.getenv("QYWX_ZB_URL")
        
        # æ£€æŸ¥ç¯å¢ƒå˜é‡
        if not self.webhook_url:
            print("è­¦å‘Šï¼šQYWX_URLç¯å¢ƒå˜é‡æœªè®¾ç½®ï¼Œæ— æ³•å‘é€å¸¸è§„é€šçŸ¥")
        if not self.webhook_zb_url:
            print("è­¦å‘Šï¼šQYWX_ZB_URLç¯å¢ƒå˜é‡æœªè®¾ç½®ï¼Œæ— æ³•å‘é€ä¸­æ ‡ç‰¹åˆ«é€šçŸ¥")
        
        # APIé…ç½®
        self.api_url = "https://ggzy.sc.yichang.gov.cn/EpointWebBuilder/rest/secaction/getSecInfoListYzm"
        self.site_guid = "7eb5f7f1-9041-43ad-8e13-8fcb82ea831a"
        self.category_num = "003001004"  # ä¸­æ ‡å€™é€‰äººç±»åˆ«
        self.page_size = 6
        self.latest_new_count = 0  # è·Ÿè¸ªæœ€æ–°æ–°å¢æ•°é‡
        self.base_url = "https://jyj.zhijiang.gov.cn"  # åŸºç¡€URL
        
    def _load_json_file(self, filename: str) -> List[Dict]:
        """åŠ è½½JSONæ–‡ä»¶"""
        try:
            if os.path.exists(filename):
                with open(filename, 'r', encoding='utf-8') as f:
                    return json.load(f)
            return []
        except Exception as e:
            print(f"[æ–‡ä»¶é”™è¯¯] åŠ è½½ {filename} å¤±è´¥: {str(e)}")
            return []

    def _save_json_file(self, filename: str, data: List[Dict]):
        """ä¿å­˜JSONæ–‡ä»¶"""
        try:
            with open(filename, 'w', encoding='utf-8') as f:
                json.dump(data, f, ensure_ascii=False, indent=2)
        except Exception as e:
            print(f"[æ–‡ä»¶é”™è¯¯] ä¿å­˜ {filename} å¤±è´¥: {str(e)}")

    def _is_existing_record(self, new_item: Dict, existing: List[Dict]) -> bool:
        """æ£€æŸ¥è®°å½•æ˜¯å¦å·²å­˜åœ¨"""
        new_id = new_item.get("infoid")
        new_url = new_item.get("infourl")
        return any(
            item.get("infoid") == new_id or 
            item.get("infourl") == new_url
            for item in existing
        )
    
    def reparse_all_data(self):
        """é‡æ–°è§£ææ‰€æœ‰åŸå§‹æ•°æ®"""
        original_data = self._load_json_file(self.original_file)
        parsed_data = []
    
        for item in original_data:
            parsed_record = {
                "infoid": item.get("infoid"),
                "infourl": item.get("infourl"),
                "parsed_data": self._parse_html_content(item),
                "raw_data": {
                    "title": item.get("title"),
                    "infodate": item.get("infodate")
                }
            }
            parsed_data.append(parsed_record)
        
        self._save_json_file(self.parsed_file, parsed_data)
        print(f"[é‡è§£æå®Œæˆ] å…±è§£æ {len(parsed_data)} æ¡æ•°æ®å¹¶ä¿å­˜åˆ° {self.parsed_file}")

    def fetch_latest_data(self) -> List[Dict]:
        """è·å–æœ€æ–°æ‹›æ ‡æ•°æ®"""
        payload = {
            "siteGuid": self.site_guid,
            "categoryNum": self.category_num,
            "pageindex": "0",
            "pagesize": str(self.page_size),
            "content": "",
            "startdate": "",
            "enddate": "", 
            "xiqucode": ""
        }
        
        for attempt in range(3):
            try:
                response = requests.post(self.api_url, data=payload, timeout=30)
                response.raise_for_status()
                return response.json().get("custom", {}).get("infodata", [])
            except requests.exceptions.Timeout:
                print(f"[ç¬¬ {attempt+1} æ¬¡å°è¯•] è¯·æ±‚è¶…æ—¶")
            except requests.RequestException as e:
                print(f"[ç¬¬ {attempt+1} æ¬¡å°è¯•] è¯·æ±‚å¤±è´¥: {str(e)}")
            time.sleep(5)
        
        print("[æœ€ç»ˆå¤±è´¥] æ— æ³•è·å–æ•°æ®")
        return []

    def process_and_store_data(self) -> int:
        """å¤„ç†å¹¶å­˜å‚¨æ•°æ®"""
        new_raw_data = self.fetch_latest_data()
        if not new_raw_data:
            return 0

        existing_raw = self._load_json_file(self.original_file)
        new_items = [
            item for item in new_raw_data
            if not self._is_existing_record(item, existing_raw)
        ]
        
        if not new_items:
            return 0

        # ä¿å­˜åŸå§‹æ•°æ®
        updated_raw = existing_raw + new_items
        self._save_json_file(self.original_file, updated_raw)
        
        # è§£ææ–°æ•°æ®
        parsed_data = self._load_json_file(self.parsed_file)
        for item in new_items:
            parsed_record = {
                "infoid": item.get("infoid"),
                "infourl": item.get("infourl"),
                "parsed_data": self._parse_html_content(item),
                "raw_data": {
                    "title": item.get("title"),
                    "infodate": item.get("infodate")
                }
            }
            parsed_data.append(parsed_record)
        
        self._save_json_file(self.parsed_file, parsed_data)
        self.latest_new_count = len(new_items)  # ä¿å­˜æœ€æ–°æ•°é‡
        return self.latest_new_count
        
        def _parse_html_content(self, data: Dict) -> Dict:
            """è§£æHTMLå†…å®¹ï¼Œæå–å…³é”®ä¿¡æ¯"""
            try:
                # æå–é¡¹ç›®åç§°
                project_name = data.get("customtitle", "").replace("ä¸­æ ‡å€™é€‰äººå…¬ç¤º", "").strip()
                
                # è§£æHTMLå†…å®¹
                infocontent = data.get("infocontent", "")
                soup = BeautifulSoup(infocontent, 'html.parser')
                
                # æå–å…¬ç¤ºæ—¶é—´
                publicity_period = ""
                pub_time_paragraph = soup.find('p', string=lambda text: "ä¸‰ã€å…¬ç¤ºæ—¶é—´" in text if text else False)
                if pub_time_paragraph:
                    next_p = pub_time_paragraph.find_next_sibling('p')
                    if next_p:
                        pub_text = next_p.get_text(strip=True)
                        if "å…¬ç¤ºæœŸä¸º" in pub_text:
                            publicity_period = pub_text.split("å…¬ç¤ºæœŸä¸º")[1].split("ã€‚")[0].strip()
                
                # æå–ä¸­æ ‡å€™é€‰äººåŠæŠ¥ä»·
                bidders = []
                prices = []
                
                # æŸ¥æ‰¾ä¸»è¦è¡¨æ ¼ï¼ˆåŒ…å«å€™é€‰äººåç§°å’ŒæŠ¥ä»·ï¼‰
                main_table = None
                for table in soup.find_all('table'):
                    headers = [th.get_text(strip=True) for th in table.find_all('th')]
                    headers += [td.get_text(strip=True) for td in table.find_all('td')]
                    
                    if "ä¸­æ ‡å€™é€‰äººåç§°" in headers and "æŠ•æ ‡æŠ¥ä»·" in headers:
                        main_table = table
                        break
                
                if main_table:
                    # æ‰¾åˆ°è¡¨å¤´è¡Œ
                    header_row = None
                    for row in main_table.find_all('tr'):
                        cells = [td.get_text(strip=True) for td in row.find_all(['th', 'td'])]
                        if "ä¸­æ ‡å€™é€‰äººåç§°" in cells:
                            header_row = row
                            break
                    
                    if header_row:
                        # ç¡®å®šåˆ—ä½ç½®
                        header_cells = [td.get_text(strip=True) for td in header_row.find_all(['th', 'td'])]
                        bidder_col = -1
                        price_col = -1
                        
                        for i, header in enumerate(header_cells):
                            if "ä¸­æ ‡å€™é€‰äººåç§°" in header:
                                bidder_col = i
                            elif "æŠ•æ ‡æŠ¥ä»·" in header:
                                price_col = i
                        
                        # æå–æ•°æ®è¡Œ
                        for row in main_table.find_all('tr'):
                            cells = row.find_all(['td'])
                            if len(cells) > max(bidder_col, price_col):
                                # æå–æŠ•æ ‡äºº
                                if bidder_col >= 0:
                                    bidder = cells[bidder_col].get_text(strip=True)
                                    # è¿‡æ»¤æ‰æ— æ•ˆæ•°æ®
                                    if len(bidder) > 2 and not any(keyword in bidder for keyword in 
                                                                ["ä¸‹æµ®ç‡", "è´¨é‡", "ç›®æ ‡", "è®¾è®¡", "æ–½å·¥"]):
                                        bidders.append(bidder)
                                
                                # æå–æŠ¥ä»·
                                if price_col >= 0:
                                    price = cells[price_col].get_text(strip=True)
                                    # è¿‡æ»¤æ‰æ— æ•ˆæ•°æ®
                                    if any(char in price for char in ["å…ƒ", "%", ".", "ä¸‡"]) and len(price) < 100:
                                        prices.append(price)
                
                # å¦‚æœæ²¡æœ‰æ‰¾åˆ°ä¸»è¦è¡¨æ ¼ï¼Œå°è¯•å¤‡ç”¨æ–¹æ³•
                if not bidders or not prices:
                    for table in soup.find_all('table'):
                        rows = table.find_all('tr')
                        if len(rows) > 1 and any("ä¸­æ ‡å€™é€‰äºº" in row.get_text() for row in rows[:2]):
                            # å°è¯•æ‰¾åˆ°æ•°æ®æœ€å¯†é›†çš„è¡Œ
                            candidate_rows = []
                            for row in rows[1:]:
                                cells = row.find_all('td')
                                if len(cells) > 2:  # è‡³å°‘3åˆ—æ•°æ®
                                    # æ£€æŸ¥å•å…ƒæ ¼å†…å®¹ç‰¹å¾
                                    valid_cells = sum(1 for cell in cells if len(cell.get_text(strip=True)) > 2)
                                    if valid_cells >= 2:
                                        candidate_rows.append(cells)
                            
                            if candidate_rows:
                                # å–å‰3-5ä¸ªå€™é€‰è¡Œ
                                for cells in candidate_rows[:5]:
                                    # æŠ•æ ‡äººé€šå¸¸æ˜¯ç¬¬ä¸€ä¸ªæœ‰æ„ä¹‰çš„å†…å®¹
                                    bidder_candidate = cells[0].get_text(strip=True)
                                    if len(bidder_candidate) > 2 and not any(keyword in bidder_candidate for keyword in 
                                                                            ["ä¸‹æµ®ç‡", "è´¨é‡", "ç›®æ ‡", "è®¾è®¡", "æ–½å·¥"]):
                                        bidders.append(bidder_candidate)
                                    
                                    # æŠ¥ä»·é€šå¸¸æ˜¯æ•°å­—æœ€å¤šçš„åˆ—
                                    for cell in cells[1:]:
                                        text = cell.get_text(strip=True)
                                        if any(char.isdigit() for char in text) and any(char in text for char in ["å…ƒ", ".", "%"]):
                                            prices.append(text)
                                            break
                
                # æ„å»ºå®Œæ•´URL
                infourl = data.get("infourl", "")
                full_url = f"{self.base_url}{infourl}" if infourl and infourl.startswith("/") else infourl
                
                return {
                    "project_name": project_name,
                    "publicity_period": publicity_period,
                    "bidders": bidders,
                    "prices": prices,
                    "full_url": full_url
                }
            except Exception as e:
                print(f"[è§£æé”™è¯¯] è§£æHTMLå†…å®¹å¤±è´¥: {str(e)}")
                return {}        

    def _build_message(self, record: Dict) -> str:
        """æ„å»ºé€šçŸ¥æ¶ˆæ¯"""
        try:
            parsed_data = record.get("parsed_data", {})
            raw_data = record.get("raw_data", {})
            
            # æ„å»ºä¸­æ ‡å€™é€‰äººè¡¨æ ¼
            markdown_table = ""
            if parsed_data.get("bidders") and parsed_data.get("prices"):
                table_header = "| åºå· | ä¸­æ ‡å€™é€‰äºº | æŠ•æ ‡æŠ¥ä»·(å…ƒ) |\n| :----- | :----: | -------: |"
                table_rows = []
                
                for i, (bidder, price) in enumerate(zip(parsed_data["bidders"], parsed_data["prices"])):
                    try:
                        # æ ¼å¼åŒ–é‡‘é¢ä¸ºåƒä½åˆ†éš”
                        formatted_price = f"{float(price.replace(',', '')):,.2f}"
                    except:
                        formatted_price = price
                    table_rows.append(f"| {i+1} | {bidder} | {formatted_price} |")
                
                markdown_table = table_header + "\n" + "\n".join(table_rows)
            
            # æ„å»ºå®Œæ•´æ¶ˆæ¯
            message = (
                "#ğŸ“¢ ä¸­æ ‡å€™é€‰äººå…¬å‘Š\n"
                f"ğŸ“œ æ ‡é¢˜ï¼š{raw_data.get('title', 'æœªçŸ¥æ ‡é¢˜')}\n"
                f"ğŸ“… æ—¥æœŸï¼š{raw_data.get('infodate', 'æœªçŸ¥æ—¥æœŸ')}\n"
                f"â³ å…¬ç¤ºæ—¶é—´ï¼š{parsed_data.get('publicity_period', '')}\n\n"
            )
            
            if markdown_table:
                message += "ğŸ† ä¸­æ ‡å€™é€‰äººåŠæŠ¥ä»·ï¼š\n" + markdown_table + "\n\n"
            
            message += f"ğŸ”— è¯¦æƒ…é“¾æ¥ï¼š{parsed_data.get('full_url', '')}"
            
            return message
        except Exception as e:
            print(f"[æ¶ˆæ¯æ„å»ºé”™è¯¯] æ„å»ºé€šçŸ¥æ¶ˆæ¯å¤±è´¥: {str(e)}")
            return ""

    def send_notifications(self):
        """å‘é€é€šçŸ¥"""
        if self.latest_new_count <= 0:
            return

        parsed_data = self._load_json_file(self.parsed_file)
        latest_parsed = parsed_data[-self.latest_new_count:]
        
        for record in latest_parsed:
            message = self._build_message(record)
            if not message:
                continue

            # å¸¸è§„é€šçŸ¥
            if self.webhook_url:
                self._send_wechat(message, self.webhook_url)
            
            # æ£€æŸ¥æ˜¯å¦æœ‰"ç››è£"ä¸­æ ‡
            if "ç››è£" in message:
                # ä¸­æ ‡ç‰¹åˆ«é€šçŸ¥
                if self.webhook_zb_url:
                    self._send_wechat(f"ã€å…¥å›´æŠ•æ ‡å€™é€‰äººé€šçŸ¥ã€‘\n{message}", self.webhook_zb_url)

    def _send_wechat(self, message: str, webhook: str):
        """å‘é€ä¼ä¸šå¾®ä¿¡é€šçŸ¥markdown_v2"""
        try:
            payload = {
                "msgtype": "markdown_v2",
                "markdown_v2": {
                    "content": message
                }
            }
            response = requests.post(webhook, json=payload, timeout=10)
            response.raise_for_status()
            print(f"[é€šçŸ¥æˆåŠŸ] å‘é€åˆ° {webhook}")
        except Exception as e:
            print(f"[é€šçŸ¥å¤±è´¥] {str(e)}")

if __name__ == "__main__":
    import sys
    monitor = BidMonitor()

    if "--reparse-all" in sys.argv:
        monitor.reparse_all_data()
    else:
        new_count = monitor.process_and_store_data()
        if new_count > 0:
            print(f"å‘ç° {new_count} æ¡æ–°å…¬å‘Š")
            monitor.send_notifications()
        else:
            print("æ²¡æœ‰æ–°æ•°æ®éœ€è¦å¤„ç†")
