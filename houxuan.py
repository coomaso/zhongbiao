import json
import requests
import datetime
import os
import re
import time
from bs4 import BeautifulSoup
from typing import List, Dict, Any
import traceback

class BidMonitor:
    def __init__(self):
        # åˆå§‹åŒ–æ–‡ä»¶è·¯å¾„
        self.original_file = "hx.json"
        self.parsed_file = "hx_parsed.json"
        
        # ä¼ä¸šå¾®ä¿¡é…ç½®
        self.webhook_url = os.getenv("QYWX_URL")
        self.webhook_zb_url = os.getenv("QYWX_ZB_URL")
        
        # æ£€æŸ¥ç¯å¢ƒå˜é‡
        if not self.webhook_url:
            print("è­¦å‘Šï¼šQYWX_URLç¯å¢ƒå˜é‡æœªè®¾ç½®ï¼Œæ— æ³•å‘é€å¸¸è§„é€šçŸ¥")
        if not self.webhook_zb_url:
            print("è­¦å‘Šï¼šQYWX_ZB_URLç¯å¢ƒå˜é‡æœªè®¾ç½®ï¼Œæ— æ³•å‘é€ä¸­æ ‡ç‰¹åˆ«é€šçŸ¥")
        
        # APIé…ç½®
        self.api_url = "https://ggzy.sc.yichang.gov.cn/EpointWebBuilder/rest/secaction/getSecInfoListYzm"
        self.site_guid = "7eb5f7f1-9041-43ad-8e13-8fcb82ea831a"
        self.category_num = "003001004"  # ä¸­æ ‡å€™é€‰äººç±»åˆ«
        self.page_size = 6
        self.latest_new_count = 0  # è·Ÿè¸ªæœ€æ–°æ–°å¢æ•°é‡
        self.base_url = "https://ggzy.sc.yichang.gov.cn"  # åŸºç¡€URL
        
    def _load_json_file(self, filename: str) -> List[Dict]:
        """åŠ è½½JSONæ–‡ä»¶"""
        try:
            if os.path.exists(filename):
                with open(filename, 'r', encoding='utf-8') as f:
                    return json.load(f)
            return []
        except Exception as e:
            print(f"[æ–‡ä»¶é”™è¯¯] åŠ è½½ {filename} å¤±è´¥: {str(e)}")
            return []

    def _save_json_file(self, filename: str, data: List[Dict]):
        """ä¿å­˜JSONæ–‡ä»¶"""
        try:
            with open(filename, 'w', encoding='utf-8') as f:
                json.dump(data, f, ensure_ascii=False, indent=2)
        except Exception as e:
            print(f"[æ–‡ä»¶é”™è¯¯] ä¿å­˜ {filename} å¤±è´¥: {str(e)}")

    def _is_existing_record(self, new_item: Dict, existing: List[Dict]) -> bool:
        """æ£€æŸ¥è®°å½•æ˜¯å¦å·²å­˜åœ¨"""
        new_id = new_item.get("infoid")
        new_url = new_item.get("infourl")
        return any(
            item.get("infoid") == new_id or 
            item.get("infourl") == new_url
            for item in existing
        )
    
    def reparse_all_data(self):
        """é‡æ–°è§£ææ‰€æœ‰åŸå§‹æ•°æ®"""
        original_data = self._load_json_file(self.original_file)
        parsed_data = []
        
        for item in original_data:
            parsed_record = {
                "infoid": item.get("infoid"),
                "infourl": item.get("infourl"),
                "parsed_data": self._parse_html_content(item),
                "raw_data": {
                    "title": item.get("title"),
                    "infodate": item.get("infodate")
                }
            }
            parsed_data.append(parsed_record)
        
        self._save_json_file(self.parsed_file, parsed_data)
        print(f"[é‡è§£æå®Œæˆ] å…±è§£æ {len(parsed_data)} æ¡æ•°æ®å¹¶ä¿å­˜åˆ° {self.parsed_file}")

    def fetch_latest_data(self) -> List[Dict]:
        """è·å–æœ€æ–°æ‹›æ ‡æ•°æ®"""
        payload = {
            "siteGuid": self.site_guid,
            "categoryNum": self.category_num,
            "pageindex": "0",
            "pagesize": str(self.page_size),
            "content": "",
            "startdate": "",
            "enddate": "", 
            "xiqucode": ""
        }
        
        for attempt in range(3):
            try:
                response = requests.post(self.api_url, data=payload, timeout=30)
                response.raise_for_status()
                return response.json().get("custom", {}).get("infodata", [])
            except requests.exceptions.Timeout:
                print(f"[ç¬¬ {attempt+1} æ¬¡å°è¯•] è¯·æ±‚è¶…æ—¶")
            except requests.RequestException as e:
                print(f"[ç¬¬ {attempt+1} æ¬¡å°è¯•] è¯·æ±‚å¤±è´¥: {str(e)}")
            time.sleep(5)
        
        print("[æœ€ç»ˆå¤±è´¥] æ— æ³•è·å–æ•°æ®")
        return []

    def process_and_store_data(self) -> int:
        """å¤„ç†å¹¶å­˜å‚¨æ•°æ®"""
        new_raw_data = self.fetch_latest_data()
        if not new_raw_data:
            return 0

        existing_raw = self._load_json_file(self.original_file)
        new_items = [
            item for item in new_raw_data
            if not self._is_existing_record(item, existing_raw)
        ]
        
        if not new_items:
            return 0

        # ä¿å­˜åŸå§‹æ•°æ®
        updated_raw = existing_raw + new_items
        self._save_json_file(self.original_file, updated_raw)
        
        # è§£ææ–°æ•°æ®
        parsed_data = self._load_json_file(self.parsed_file)
        for item in new_items:
            parsed_record = {
                "infoid": item.get("infoid"),
                "infourl": item.get("infourl"),
                "parsed_data": self._parse_html_content(item),
                "raw_data": {
                    "title": item.get("title"),
                    "infodate": item.get("infodate")
                }
            }
            parsed_data.append(parsed_record)
        
        self._save_json_file(self.parsed_file, parsed_data)
        self.latest_new_count = len(new_items)  # ä¿å­˜æœ€æ–°æ•°é‡
        return self.latest_new_count

    def _parse_html_content(self, data: Dict) -> Dict:
        """è§£æHTMLå†…å®¹ï¼Œæå–é¡¹ç›®ä¿¡æ¯å’Œä¸­æ ‡å€™é€‰äººåˆ—è¡¨"""
        project_name = ""
        try:
            project_name = data.get("customtitle", "").replace("ä¸­æ ‡å€™é€‰äººå…¬ç¤º", "").strip()
            infocontent = data.get("infocontent", "")
            soup = BeautifulSoup(infocontent, 'html.parser')
            full_text = soup.get_text()

            # æå–å…¬ç¤ºæ—¶é—´ - å¢å¼ºåŒ¹é…é€»è¾‘
            publicity_period = ""
            pub_patterns = [
                r"å…¬ç¤º[æœŸæ—¶]ä¸º?[:ï¼š]?\s*(.+?è‡³.+?)\s*(?:\n|<|$)",
                r"å…¬ç¤ºæ—¶é—´[:ï¼š]?\s*(.+?è‡³.+?)\s*(?:\n|<|$)",
                r"å…¬ç¤ºæœŸ[:ï¼š]?\s*(.+?è‡³.+?)\s*(?:\n|<|$)",
                r"å…¬ç¤ºæœŸä¸º(\d{4}å¹´\d{1,2}æœˆ\d{1,2}æ—¥ \d{1,2}æ—¶\d{1,2}åˆ†è‡³\d{4}å¹´\d{1,2}æœˆ\d{1,2}æ—¥ \d{1,2}æ—¶\d{1,2}åˆ†)",
                r"å…¬ç¤ºæœŸ[ä¸º]?(\d{4}å¹´\d{1,2}æœˆ\d{1,2}æ—¥ \d{1,2}æ—¶\d{1,2}åˆ†è‡³\d{4}å¹´\d{1,2}æœˆ\d{1,2}æ—¥ \d{1,2}æ—¶\d{1,2}åˆ†)"
            ]
            for pattern in pub_patterns:
                match = re.search(pattern, full_text)
                if match:
                    if pattern in [pub_patterns[3], pub_patterns[4]]:  # å¤„ç†ç‰¹å®šæ ¼å¼çš„æ—¶é—´
                        publicity_period = match.group(0).replace("å…¬ç¤ºæœŸä¸º", "").strip()
                    else:
                        publicity_period = match.group(1).strip()
                    break

            bidders_and_prices = []

            # æ–¹æ³•1ï¼šç²¾ç¡®æå–è¡¨æ ¼ä¸­çš„å€™é€‰äººåŠæŠ¥ä»· - å¢å¼ºè¡¨æ ¼è§£æ
            for table in soup.find_all('table'):
                # æŸ¥æ‰¾åŒ…å«"ä¸­æ ‡å€™é€‰äººåç§°"çš„è¡Œ
                header_found = False
                for row in table.find_all('tr'):
                    row_text = row.get_text(strip=True)
                    if any(keyword in row_text for keyword in ["ä¸­æ ‡å€™é€‰äººåç§°", "å€™é€‰äººåç§°", "å•ä½åç§°", "åæ¬¡"]):
                        header_found = True
                        
                        # å°è¯•ä»å½“å‰è¡Œæˆ–ä¸‹ä¸€è¡Œæå–å€™é€‰äººæ•°æ®
                        candidate_row = row
                        # å¦‚æœå½“å‰è¡Œæ²¡æœ‰è¶³å¤Ÿçš„å•å…ƒæ ¼ï¼Œå°è¯•ä¸‹ä¸€è¡Œ
                        if len(row.find_all(['td', 'th'])) < 3:
                            candidate_row = row.find_next_sibling('tr')
                        
                        if candidate_row:
                            candidate_cells = candidate_row.find_all(['td', 'th'])
                            # è·³è¿‡è¡¨å¤´å•å…ƒæ ¼ï¼ˆé€šå¸¸æ˜¯å‰ä¸¤ä¸ªå•å…ƒæ ¼ï¼‰
                            candidates = []
                            # ç¡®å®šèµ·å§‹åˆ—ï¼šå¦‚æœç¬¬ä¸€åˆ—åŒ…å«"ç¬¬ä¸€å"ç­‰ï¼Œåˆ™ä»ç¬¬ä¸€åˆ—å¼€å§‹
                            start_col = 0
                            if "åæ¬¡" in row_text or "ç¬¬ä¸€å" in candidate_cells[0].get_text(strip=True):
                                start_col = 1  # è·³è¿‡åæ¬¡åˆ—
                            
                            for i in range(start_col, len(candidate_cells)):
                                text = candidate_cells[i].get_text(strip=True)
                                # æ’é™¤ç©ºå€¼ã€æ— å…³æ–‡æœ¬å’Œåæ¬¡æ–‡æœ¬
                                if (text and len(text) > 1 and 
                                    not re.match(r'^ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å\d]+å?$', text) and
                                    ("å…¬å¸" in text or "é›†å›¢" in text or "æœ‰é™" in text or "è®¾è®¡é™¢" in text)):
                                    candidates.append(text)
                        
                        # æŸ¥æ‰¾åŒ…å«"æŠ•æ ‡æŠ¥ä»·"çš„è¡Œ
                        price_row = None
                        for next_row in row.find_next_siblings('tr'):
                            if any(keyword in next_row.get_text() for keyword in 
                                  ["æŠ•æ ‡æŠ¥ä»·", "æŠ¥ä»·", "æŠ•æ ‡æ€»ä»·", "æ€»æŠ¥ä»·", "æŠ•æ ‡é‡‘é¢", "é‡‘é¢"]):
                                price_row = next_row
                                break
                        
                        if price_row:
                            price_cells = price_row.find_all(['td', 'th'])
                            prices = []
                            # ä½¿ç”¨ç›¸åŒçš„èµ·å§‹åˆ—
                            for i in range(start_col, len(price_cells)):
                                text = price_cells[i].get_text(strip=True)
                                # ä¿ç•™æ‰€æœ‰æ–‡æœ¬å†…å®¹ï¼ˆå¯èƒ½æ˜¯æ•°å­—æˆ–æè¿°æ€§æ–‡æœ¬ï¼‰
                                if text and text != "/" and not re.match(r'^ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å\d]+å?$', text):
                                    prices.append(text)
                            
                            # é…å¯¹å€™é€‰äººå’ŒæŠ¥ä»·
                            for i, candidate in enumerate(candidates):
                                price = prices[i] if i < len(prices) else "æœªæä¾›"
                                bidders_and_prices.append({
                                    "bidder": candidate,
                                    "price": price
                                })
                        
                        # å¦‚æœæ‰¾åˆ°å€™é€‰äººï¼Œè·³å‡ºå¾ªç¯
                        if bidders_and_prices:
                            break
                if header_found:
                    break

            # æ–¹æ³•2ï¼šå¦‚æœè¡¨æ ¼æå–å¤±è´¥ï¼Œå°è¯•ä»æ–‡æœ¬ä¸­æå–
            if not bidders_and_prices:
                # æŸ¥æ‰¾è¯„å®¡ç»“æœéƒ¨åˆ†
                review_section = ""
                # å°è¯•å¤šç§å¯èƒ½çš„ç« èŠ‚åˆ†éš”
                section_patterns = [
                    r'äºŒã€è¯„æ ‡ç»“æœ(.+?)ä¸‰ã€å…¬ç¤ºæ—¶é—´',
                    r'äºŒã€è¯„æ ‡æƒ…å†µ(.+?)ä¸‰ã€å…¬ç¤ºæ—¶é—´',
                    r'äºŒã€è¯„å®¡ç»“æœ(.+?)ä¸‰ã€å…¬ç¤ºæ—¶é—´',
                    r'äºŒã€ä¸­æ ‡å€™é€‰äºº(.+?)ä¸‰ã€å…¬ç¤ºæ—¶é—´'
                ]
                for pattern in section_patterns:
                    review_match = re.search(pattern, full_text, re.DOTALL)
                    if review_match:
                        review_section = review_match.group(1)
                        break
                if not review_section:
                    review_section = full_text
                
                # æå–å€™é€‰äººåç§° - å¢å¼ºæ¨¡å¼
                candidates = []
                # æ¨¡å¼1ï¼šåŒ¹é…"ç¬¬Xä¸­æ ‡å€™é€‰äººï¼šå…¬å¸åç§°"
                candidate_pattern1 = r'ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å\d]+ä¸­æ ‡å€™é€‰äºº[ï¼š:\s]*([^\n]+)'
                candidate_matches1 = re.findall(candidate_pattern1, review_section)
                if candidate_matches1:
                    candidates = [match.strip() for match in candidate_matches1]
                else:
                    # æ¨¡å¼2ï¼šåŒ¹é…"ä¸­æ ‡å€™é€‰äººåç§°ï¼šå…¬å¸A,å…¬å¸B,å…¬å¸C"
                    candidate_pattern2 = r'ä¸­æ ‡å€™é€‰äººåç§°[ï¼š:\s]*([^\n]+)'
                    candidate_match2 = re.search(candidate_pattern2, review_section)
                    if candidate_match2:
                        candidates_text = candidate_match2.group(1)
                        # åˆ†å‰²å€™é€‰äººåç§°
                        candidates = re.split(r'[ã€ï¼Œ,;ï¼›]', candidates_text)
                        # æ¸…ç†ç©ºæ ¼
                        candidates = [c.strip() for c in candidates if c.strip()]
                    else:
                        # æ¨¡å¼3ï¼šç›´æ¥æŸ¥æ‰¾æ’åä¸åˆ†å…ˆåçš„å€™é€‰äºº
                        unordered_pattern = r'ä¸­æ ‡å€™é€‰äººä¸º[ï¼ˆ(]æ’åä¸åˆ†å…ˆå[ï¼‰)]?[ï¼š:\s]*([^\n]+)'
                        unordered_match = re.search(unordered_pattern, review_section)
                        if unordered_match:
                            candidates_text = unordered_match.group(1)
                            # åˆ†å‰²å€™é€‰äººåç§°
                            candidates = re.split(r'[ã€ï¼Œ,;ï¼›]', candidates_text)
                            # æ¸…ç†ç©ºæ ¼
                            candidates = [c.strip() for c in candidates if c.strip()]
                        else:
                            # æ¨¡å¼4ï¼šå°è¯•æå–è¡¨æ ¼å¤–çš„å€™é€‰äºº
                            table_candidates = []
                            for row in soup.find_all('tr'):
                                cells = row.find_all(['td', 'th'])
                                for cell in cells:
                                    text = cell.get_text(strip=True)
                                    if ("å…¬å¸" in text or "é›†å›¢" in text or "æœ‰é™" in text) and len(text) > 5:
                                        if not any(c == text for c in table_candidates):
                                            table_candidates.append(text)
                            if table_candidates:
                                candidates = table_candidates
                            else:
                                # å¤‡é€‰æ–¹æ¡ˆï¼šæå–æ‰€æœ‰å…¬å¸åç§°
                                company_pattern = r'([\u4e00-\u9fa5]{2,}(?:å…¬å¸|é›†å›¢|è®¾è®¡é™¢|ç ”ç©¶é™¢|å·¥ç¨‹å±€|æœ‰é™å…¬å¸|è‚¡ä»½å…¬å¸))'
                                candidates = re.findall(company_pattern, review_section)
                                # å»é‡
                                seen = set()
                                unique_candidates = [c for c in candidates if c not in seen and not seen.add(c)]
                                candidates = unique_candidates
                
                # æå–æŠ¥ä»· - å¢å¼ºæŠ¥ä»·æ¨¡å¼
                prices = []
                # æŸ¥æ‰¾æŠ•æ ‡æŠ¥ä»·éƒ¨åˆ†
                price_pattern = r'(?:æŠ•æ ‡æŠ¥ä»·|æŠ¥ä»·|æŠ•æ ‡æ€»ä»·|æ€»æŠ¥ä»·)[ï¼š:\s]*([^\n]+?)(?:\n|$)'
                price_matches = re.findall(price_pattern, review_section)
                if price_matches:
                    # ä»åŒ¹é…çš„æ–‡æœ¬ä¸­æå–å…·ä½“çš„æŠ¥ä»·å€¼
                    for match in price_matches:
                        # å°è¯•æå–æ•°å­—å’Œå•ä½
                        price_values = re.findall(r'([\d,.]+[ä¸‡å…ƒ%]?|[\d,.]+å…ƒ|[\d.]+%)', match)
                        if price_values:
                            prices.extend(price_values)
                else:
                    # å¤‡é€‰æ–¹æ¡ˆ1ï¼šæå–ç™¾åˆ†æ¯”è´¹ç‡
                    rate_pattern = r'æŒ‰.+?æ”¶è´¹æ ‡å‡†çš„(\d+)%'
                    rate_matches = re.findall(rate_pattern, review_section)
                    if rate_matches:
                        prices = [f"{rate}%" for rate in rate_matches]
                    else:
                        # å¤‡é€‰æ–¹æ¡ˆ2ï¼šæå–æ‰€æœ‰æ•°å­—æŠ¥ä»·
                        price_pattern = r'([\d,.]+ä¸‡å…ƒ?|[\d,.]+å…ƒ|[\d.]+%)'
                        prices = re.findall(price_pattern, review_section)
                
                # é…å¯¹å€™é€‰äººå’ŒæŠ¥ä»·
                for i, candidate in enumerate(candidates):
                    price = prices[i] if i < len(prices) else "æœªæä¾›"
                    bidders_and_prices.append({
                        "bidder": candidate,
                        "price": price
                    })

            # ç¡®ä¿è‡³å°‘æå–åˆ°3åå€™é€‰äººï¼ˆå¦‚æœåŸæ–‡æœ‰3åï¼‰
            if len(bidders_and_prices) < 3:
                # å°è¯•ä»è¡¨æ ¼ä¸­ç›´æ¥æå–æ‰€æœ‰å…¬å¸åç§°
                all_companies = []
                for table in soup.find_all('table'):
                    for row in table.find_all('tr'):
                        for cell in row.find_all(['td', 'th']):
                            text = cell.get_text(strip=True)
                            if ("å…¬å¸" in text or "é›†å›¢" in text) and len(text) > 5:
                                if not any(c == text for c in all_companies):
                                    all_companies.append(text)
                
                # å¦‚æœæ‰¾åˆ°æ›´å¤šå€™é€‰äººï¼Œåˆå¹¶ç»“æœ
                if len(all_companies) > len(bidders_and_prices):
                    for i, company in enumerate(all_companies):
                        if i >= len(bidders_and_prices):
                            # ä¸ºæ–°å‘ç°çš„å€™é€‰äººæ·»åŠ é»˜è®¤æŠ¥ä»·
                            bidders_and_prices.append({
                                "bidder": company,
                                "price": "æœªæä¾›"
                            })

            # æ„å»ºæœ€ç»ˆæ•°æ®ç»“æ„
            infourl = data.get("infourl", "")
            full_url = f"{self.base_url}{infourl}" if infourl.startswith("/") else infourl

            return {
                "project_name": project_name or "æœªçŸ¥é¡¹ç›®",
                "publicity_period": publicity_period,
                "bidders_and_prices": bidders_and_prices,
                "full_url": full_url
            }

        except Exception as e:
            print(f"[è§£æé”™è¯¯] {str(e)}")
            traceback.print_exc()
            return {
                "project_name": project_name or "è§£æå¤±è´¥",
                "publicity_period": "",
                "bidders_and_prices": [{"bidder": "è§£æå¤±è´¥", "price": "è¯·æŸ¥çœ‹è¯¦æƒ…"}],
                "full_url": ""
            }
            
    def _build_message(self, record: Dict) -> str:
        """æ„å»ºé€šçŸ¥æ¶ˆæ¯"""
        try:
            parsed_data = record.get("parsed_data", {})
            raw_data = record.get("raw_data", {})
            
            # æ„å»ºä¸­æ ‡å€™é€‰äººè¡¨æ ¼
            markdown_table = ""
            bap = parsed_data.get("bidders_and_prices", [])
            
            if bap:
                table_header = "|ä¸­æ ‡å€™é€‰äºº|æŠ•æ ‡æŠ¥ä»·|\n| :----: | :------ |"
                table_rows = []
                
                for i, item in enumerate(bap):
                    bidder = item.get("bidder", "æœªæä¾›").replace("&nbsp;", "").strip()
                    price = item.get("price", "æœªæä¾›").replace("&nbsp;", "").strip()
                    
                    # æ ¼å¼åŒ–æŠ¥ä»· - å¤„ç†å„ç§å¤æ‚æƒ…å†µ
                    formatted_price = price
                    
                    # æƒ…å†µ1ï¼šçº¯æ•°å­—ï¼ˆå¯èƒ½åŒ…å«é€—å·ï¼‰
                    if re.match(r'^[\d,]+(?:\.\d+)?$', price.replace(',', '')):
                        try:
                            # ç§»é™¤é€—å·åè½¬æ¢ä¸ºæµ®ç‚¹æ•°
                            price_num = float(price.replace(',', ''))
                            if price_num >= 1000000:  # è¶…è¿‡100ä¸‡
                                formatted_price = f"{price_num/10000:,.2f}ä¸‡å…ƒ"
                            elif price_num >= 10000:  # 1ä¸‡-100ä¸‡
                                formatted_price = f"{price_num/10000:,.2f}ä¸‡å…ƒ"
                            else:
                                formatted_price = f"{price_num:,.2f}å…ƒ"
                        except:
                            pass
                    
                    # æƒ…å†µ2ï¼šç™¾åˆ†æ¯”è´¹ç‡
                    elif '%' in price:
                        # ä¿æŒåŸæ ·æ˜¾ç¤º
                        formatted_price = price
                    
                    # æƒ…å†µ3ï¼šåŒ…å«"å…ƒ"æˆ–"ä¸‡å…ƒ"
                    elif "å…ƒ" in price or "ä¸‡å…ƒ" in price:
                        # å°è¯•æå–æ•°å­—éƒ¨åˆ†è¿›è¡Œæ ¼å¼åŒ–
                        num_match = re.search(r'([\d,\.]+)', price)
                        if num_match:
                            num_str = num_match.group(1).replace(',', '')
                            try:
                                num_val = float(num_str)
                                if "ä¸‡å…ƒ" in price or num_val >= 10000:
                                    formatted_price = f"{num_val/10000:,.2f}ä¸‡å…ƒ"
                                else:
                                    formatted_price = f"{num_val:,.2f}å…ƒ"
                            except:
                                formatted_price = price
                    
                    # æƒ…å†µ4ï¼šå¤æ‚çš„æ–‡æœ¬æè¿°ï¼ˆå¦‚æŒ‰æ”¶è´¹æ ‡å‡†ï¼‰
                    elif "æŒ‰" in price and "æ ‡å‡†" in price:
                        # ç®€åŒ–æ˜¾ç¤º
                        simplified = re.sub(r'è®¡è´¹é¢ä»¥.*', '', price)
                        formatted_price = simplified.strip()
                    
                    table_rows.append(f"|{bidder}|{formatted_price}|")
                
                markdown_table = table_header + "\n" + "\n".join(table_rows)
            
            # æ„å»ºå®Œæ•´æ¶ˆæ¯
            message = (
                "## ğŸ“¢ ä¸­æ ‡å€™é€‰äººå…¬å‘Š\n\n"
                f">**ğŸ“œ æ ‡é¢˜**ï¼š{raw_data.get('title', 'æœªçŸ¥æ ‡é¢˜')}\n\n"
                f">**ğŸ“… æ—¥æœŸ**ï¼š{raw_data.get('infodate', 'æœªçŸ¥æ—¥æœŸ')}\n\n"
                f">**â³ å…¬ç¤ºæ—¶é—´**ï¼š{parsed_data.get('publicity_period', 'æœªæä¾›')}\n\n"
            )            
            if markdown_table:
                message += "**ğŸ† ä¸­æ ‡å€™é€‰äººåŠæŠ¥ä»·ï¼š**\n" + markdown_table + "\n\n"
            else:
                message += "**ğŸ† ä¸­æ ‡å€™é€‰äººï¼š**\n"
                for i, item in enumerate(bap):
                    bidder = item.get("bidder", "æœªæä¾›")
                    price = item.get("price", "æœªæä¾›")
                    message += f"{i+1}. {bidder}"
                    if price and price != "æœªæä¾›":
                        message += f" (æŠ¥ä»·: {price})"
                    message += "\n"
                message += "\n"
            
            # æ·»åŠ å€™é€‰äººæ•°é‡ä¿¡æ¯
            if bap:
                message += f"**å…±å‘ç° {len(bap)} åä¸­æ ‡å€™é€‰äºº**\n\n"
            
            message += f"ğŸ”— **è¯¦æƒ…é“¾æ¥**ï¼š{parsed_data.get('full_url', '')}"
            
            return message
        except Exception as e:
            print(f"[æ¶ˆæ¯æ„å»ºé”™è¯¯] æ„å»ºé€šçŸ¥æ¶ˆæ¯å¤±è´¥: {str(e)}")
            traceback.print_exc()
            return ""

    def send_notifications(self):
        """å‘é€é€šçŸ¥"""
        if self.latest_new_count <= 0:
            return

        parsed_data = self._load_json_file(self.parsed_file)
        # ç¡®ä¿åªå¤„ç†å½“å‰æ–°å¢çš„æ•°æ®
        latest_parsed = parsed_data[-self.latest_new_count:]
        
        for record in latest_parsed:
            message = self._build_message(record)
            if not message:
                continue

            # å¸¸è§„é€šçŸ¥
            if self.webhook_url:
                self._send_wechat(message, self.webhook_url)
            
            # æ£€æŸ¥æ˜¯å¦æœ‰"ç››è£"ä¸­æ ‡
            if "ç››è£" in message:
                # ä¸­æ ‡ç‰¹åˆ«é€šçŸ¥
                if self.webhook_zb_url:
                    self._send_wechat(f"ã€å…¥å›´æŠ•æ ‡å€™é€‰äººé€šçŸ¥ã€‘\n{message}", self.webhook_zb_url)

    def _send_wechat(self, message: str, webhook: str):
        """å‘é€ä¼ä¸šå¾®ä¿¡é€šçŸ¥"""
        try:
            payload = {
                "msgtype": "markdown_v2",
                "markdown_v2": {
                    "content": message
                }
            }
            response = requests.post(webhook, json=payload, timeout=10)
            response.raise_for_status()
            print(f"[é€šçŸ¥æˆåŠŸ] å‘é€åˆ° {webhook}")
        except Exception as e:
            print(f"[é€šçŸ¥å¤±è´¥] {str(e)}")

if __name__ == "__main__":
    import sys
    monitor = BidMonitor()

    if "--reparse-all" in sys.argv:
        monitor.reparse_all_data()
    else:
        new_count = monitor.process_and_store_data()
        if new_count > 0:
            print(f"å‘ç° {new_count} æ¡æ–°å…¬å‘Š")
            monitor.send_notifications()
        else:
            print("æ²¡æœ‰æ–°æ•°æ®éœ€è¦å¤„ç†")
