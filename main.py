import json
import requests
import datetime
import os
import re
import time
from bs4 import BeautifulSoup
from typing import List, Dict, Any

class BidMonitor:
    def __init__(self):
        # åˆå§‹åŒ–æ–‡ä»¶è·¯å¾„
        self.original_file = "zb.json"
        self.parsed_file = "parsed.json"
        
        # ä¼ä¸šå¾®ä¿¡é…ç½®
        self.webhook_url = os.getenv("QYWX_URL")
        self.webhook_zb_url = os.getenv("QYWX_ZB_URL")
        
        # æ£€æŸ¥ç¯å¢ƒå˜é‡
        if not self.webhook_url:
            print("è­¦å‘Šï¼šQYWX_URLç¯å¢ƒå˜é‡æœªè®¾ç½®ï¼Œæ— æ³•å‘é€å¸¸è§„é€šçŸ¥")
        if not self.webhook_zb_url:
            print("è­¦å‘Šï¼šQYWX_ZB_URLç¯å¢ƒå˜é‡æœªè®¾ç½®ï¼Œæ— æ³•å‘é€ä¸­æ ‡ç‰¹åˆ«é€šçŸ¥")
        
        # APIé…ç½®
        self.api_url = "https://ggzy.sc.yichang.gov.cn/EpointWebBuilder/rest/secaction/getSecInfoListYzm"
        self.site_guid = "7eb5f7f1-9041-43ad-8e13-8fcb82ea831a"
        self.category_num = "003001005"
        self.page_size = 6
        self.latest_new_count = 0  # è·Ÿè¸ªæœ€æ–°æ–°å¢æ•°é‡

    def reparse_all_data(self):
        """é‡æ–°è§£ææ‰€æœ‰åŸå§‹æ•°æ®"""
        original_data = self._load_json_file(self.original_file)
        parsed_data = []
    
        for item in original_data:
            parsed_record = {
                "infoid": item.get("infoid"),
                "infourl": item.get("infourl"),
                "parsed_data": self._parse_html_content(item.get("infocontent", "")),
                "raw_data": {
                    "title": item.get("title"),
                    "infodate": item.get("infodate")
                }
            }
            parsed_data.append(parsed_record)
        
        self._save_json_file(self.parsed_file, parsed_data)
        print(f"[é‡è§£æå®Œæˆ] å…±è§£æ {len(parsed_data)} æ¡æ•°æ®å¹¶ä¿å­˜åˆ° {self.parsed_file}")

    def fetch_latest_data(self) -> List[Dict]:
        """è·å–æœ€æ–°æ‹›æ ‡æ•°æ®"""
        payload = {
            "siteGuid": self.site_guid,
            "categoryNum": self.category_num,
            "pageindex": "0",
            "pagesize": str(self.page_size),
            "content": "",
            "startdate": "",
            "enddate": "", 
            "xiqucode": ""
        }
        
        for attempt in range(3):
            try:
                response = requests.post(self.api_url, data=payload, timeout=30)
                response.raise_for_status()
                return response.json().get("custom", {}).get("infodata", [])
            except requests.exceptions.Timeout:
                print(f"[ç¬¬ {attempt+1} æ¬¡å°è¯•] è¯·æ±‚è¶…æ—¶")
            except requests.RequestException as e:
                print(f"[ç¬¬ {attempt+1} æ¬¡å°è¯•] è¯·æ±‚å¤±è´¥: {str(e)}")
            time.sleep(5)
        
        print("[æœ€ç»ˆå¤±è´¥] æ— æ³•è·å–æ•°æ®")
        return []

    def process_and_store_data(self) -> int:
        """å¤„ç†å¹¶å­˜å‚¨æ•°æ®"""
        new_raw_data = self.fetch_latest_data()
        if not new_raw_data:
            return 0

        existing_raw = self._load_json_file(self.original_file)
        new_items = [
            item for item in new_raw_data
            if not self._is_existing_record(item, existing_raw)
        ]
        
        if not new_items:
            return 0

        # ä¿å­˜åŸå§‹æ•°æ®
        updated_raw = existing_raw + new_items
        self._save_json_file(self.original_file, updated_raw)
        
        # è§£ææ–°æ•°æ®
        parsed_data = self._load_json_file(self.parsed_file)
        for item in new_items:
            parsed_record = {
                "infoid": item.get("infoid"),
                "infourl": item.get("infourl"),
                "parsed_data": self._parse_html_content(item.get("infocontent", "")),
                "raw_data": {
                    "title": item.get("title"),
                    "infodate": item.get("infodate")
                }
            }
            parsed_data.append(parsed_record)
        
        self._save_json_file(self.parsed_file, parsed_data)
        self.latest_new_count = len(new_items)  # ä¿å­˜æœ€æ–°æ•°é‡
        return self.latest_new_count

    def send_notifications(self):
        """å‘é€é€šçŸ¥"""
        if self.latest_new_count <= 0:
            return

        parsed_data = self._load_json_file(self.parsed_file)
        latest_parsed = parsed_data[-self.latest_new_count:]
        
        for record in latest_parsed:
            message = self._build_message(record)
            if not message:
                continue

            # å¸¸è§„é€šçŸ¥
            if self.webhook_url:
                self._send_wechat(message, self.webhook_url)
            
            # ä¸­æ ‡ç‰¹åˆ«é€šçŸ¥
            if "ç››è£" in record.get("parsed_data", {}).get("ä¸­æ ‡äºº", ""):
                if self.webhook_zb_url:
                    self._send_wechat(f"ã€ä¸­æ ‡é€šçŸ¥ã€‘\n{message}", self.webhook_zb_url)

    def _parse_html_content(self, html: str) -> Dict:
        """è§£æHTMLè¡¨æ ¼"""
        result = {}
        try:
            soup = BeautifulSoup(html, 'html.parser')
            table = soup.find("table")
            if not table:
                return result

            for row in table.find_all("tr"):
                cols = [td.get_text(strip=True) for td in row.find_all("td")]
                self._process_table_row(cols, result)

            # å¤‡ç”¨è§£ææ–¹å¼
            if not result.get("ä¸­æ ‡äºº"):
                result["ä¸­æ ‡äºº"] = self._fallback_extract(html, r"ä¸­æ ‡(äºº|å•ä½)")
                
        except Exception as e:
            print(f"[è§£æé”™è¯¯] {str(e)}")
        return result

    def _process_table_row(self, columns: List[str], result: Dict):
        """å¤„ç†è¡¨æ ¼è¡Œ"""
        if len(columns) >= 2:
            key = self._normalize_key(columns[0])
            result[key] = columns[1]
        if len(columns) >= 4:
            key = self._normalize_key(columns[2])
            result[key] = columns[3]

    def _normalize_key(self, text: str) -> str:
        """æ ‡å‡†åŒ–é”®å"""
        return re.sub(r'[:ï¼š\s]+', '', text).strip()

    def _build_message(self, record: Dict) -> str:
        """æ„å»ºæ¶ˆæ¯æ¨¡æ¿"""
        parsed = record.get("parsed_data", {})
        raw = record.get("raw_data", {})
        
        # åŠ¨æ€å­—æ®µåŒ¹é…
        bidder = self._find_field(parsed, r"ä¸­æ ‡(äºº|å•ä½)")
        price = self._find_field(parsed, r"ä¸­æ ‡(ä»·|é‡‘é¢)")
        
        return (
            f"ğŸ“¢ æ–°ä¸­æ ‡å…¬å‘Š\n"
            f"----------------------------\n"
            f"â–ª æ ‡é¢˜ï¼š{raw.get('title', 'æœªçŸ¥æ ‡é¢˜')}\n"
            f"â–ª æ—¥æœŸï¼š{raw.get('infodate', 'æœªçŸ¥æ—¥æœŸ')}\n"
            f"â–ª ä¸­æ ‡æ–¹ï¼š{bidder}\n"
            f"â–ª ä¸­æ ‡é‡‘é¢ï¼š{price}\n"
            f"ğŸ”— è¯¦æƒ…é“¾æ¥ï¼š{self._build_full_url(record.get('infourl', ''))}\n"
            f"----------------------------"
        )

    def _find_field(self, data: Dict, pattern: str) -> str:
        """æ­£åˆ™åŒ¹é…å­—æ®µ"""
        for key in data:
            if re.search(pattern, key):
                return data[key]
        return self._fallback_extract(data.get("raw_html", ""), pattern)

    def _fallback_extract(self, html: str, pattern: str) -> str:
        """å¤‡ç”¨è§£ææ–¹æ³•"""
        try:
            soup = BeautifulSoup(html, 'html.parser')
            td = soup.find(string=re.compile(pattern))
            return td.find_next('td').get_text(strip=True) if td else "æœªæ‰¾åˆ°"
        except:
            return "è§£æå¤±è´¥"

    def _build_full_url(self, path: str) -> str:
        """æ„å»ºå®Œæ•´URL"""
        if not path.startswith("/"):
            return "é“¾æ¥æ— æ•ˆ"
        return f"https://ggzy.sc.yichang.gov.cn{path}"

    def _send_wechat(self, message: str, webhook: str):
        """å‘é€ä¼ä¸šå¾®ä¿¡é€šçŸ¥"""
        payload = {
            "msgtype": "text",
            "text": {"content": message}
        }
        try:
            response = requests.post(webhook, json=payload, timeout=10)
            response.raise_for_status()
            print(f"[é€šçŸ¥æˆåŠŸ] å‘é€åˆ° {webhook}")
        except Exception as e:
            print(f"[é€šçŸ¥å¤±è´¥] {str(e)}")

    # å…¶ä»–è¾…åŠ©æ–¹æ³•ä¿æŒä¸å˜...
    
if __name__ == "__main__":
    import sys
    monitor = BidMonitor()

    if "--reparse-all" in sys.argv:
        monitor.reparse_all_data()
    else:
        new_count = monitor.process_and_store_data()
        if new_count > 0:
            print(f"å‘ç° {new_count} æ¡æ–°å…¬å‘Š")
            monitor.send_notifications()
        else:
            print("æ²¡æœ‰æ–°æ•°æ®éœ€è¦å¤„ç†")
